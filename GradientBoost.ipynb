{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_curve, auc\n",
    "from imblearn.pipeline import Pipeline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.feature_selection import SelectKBest, f_classif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./dataset/processed_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "X = df.drop('diabetes', axis=1)\n",
    "y = df['diabetes']\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('feature_selection', SelectKBest(f_classif, k=7)),  \n",
    "    ('smote', SMOTE(random_state=42, sampling_strategy=0.5)),  \n",
    "    ('gb', GradientBoostingClassifier(random_state=42))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_dist = {\n",
    "    'gb__n_estimators': [100, 200],  \n",
    "    'gb__learning_rate': [0.1, 0.3],\n",
    "    'gb__max_depth': [3, 4],  \n",
    "    'gb__min_samples_split': [5],  \n",
    "    'gb__subsample': [0.8],  \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform GridSearch\n",
    "random_search = RandomizedSearchCV(\n",
    "    pipeline,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=4,  # Số lượng combinations được thử\n",
    "    cv=3,  # Giảm số fold trong cross-validation\n",
    "    scoring='f1',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Fit model\n",
    "random_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get best model\n",
    "best_model = random_search.best_estimator_\n",
    "y_pred = best_model.predict(X_test)\n",
    "y_pred_proba = best_model.predict_proba(X_test)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print model evaluation\n",
    "print(\"\\nBest Parameters:\", random_search.best_params_)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ROC Curve\n",
    "plt.figure(figsize=(10, 6))\n",
    "fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve - Gradient Boosting Model')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('Confusion Matrix - Gradient Boosting Model')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Importance\n",
    "feature_selector = best_model.named_steps['feature_selection']\n",
    "selected_features_mask = feature_selector.get_support()\n",
    "selected_features = X.columns[selected_features_mask].tolist()\n",
    "\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': selected_features,\n",
    "    'importance': best_model.named_steps['gb'].feature_importances_\n",
    "})\n",
    "feature_importance = feature_importance.sort_values('importance', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x='importance', y='feature', data=feature_importance)\n",
    "plt.title('Feature Importance (Gradient Boosting)')\n",
    "plt.xlabel('Importance Score')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning Curves\n",
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "train_sizes, train_scores, val_scores = learning_curve(\n",
    "    best_model, X_train, y_train, cv=5,\n",
    "    train_sizes=np.linspace(0.1, 1.0, 10),\n",
    "    scoring='accuracy'\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(train_sizes, np.mean(train_scores, axis=1), label='Training score')\n",
    "plt.plot(train_sizes, np.mean(val_scores, axis=1), label='Cross-validation score')\n",
    "plt.xlabel('Training Size')\n",
    "plt.ylabel('Accuracy Score')\n",
    "plt.title('Learning Curves - Gradient Boosting Model')\n",
    "plt.legend(loc='best')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training deviance\n",
    "test_score = np.zeros((params['gb__n_estimators'],), dtype=np.float64)\n",
    "for i, y_pred in enumerate(best_model.named_steps['gb'].staged_predict(X_test)):\n",
    "    test_score[i] = best_model.named_steps['gb'].loss_(y_test, y_pred)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(np.arange(params['gb__n_estimators']) + 1, best_model.named_steps['gb'].train_score_,\n",
    "         'b-', label='Training Set Deviance')\n",
    "plt.plot(np.arange(params['gb__n_estimators']) + 1, test_score, 'r-',\n",
    "         label='Test Set Deviance')\n",
    "plt.legend(loc='upper right')\n",
    "plt.xlabel('Boosting Iterations')\n",
    "plt.ylabel('Deviance')\n",
    "plt.title('Training and Test Deviance vs Boosting Iterations')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-validation Scores Distribution\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "cv_scores = cross_val_score(best_model, X_train, y_train, cv=10, scoring='accuracy')\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.hist(cv_scores, bins=20)\n",
    "plt.axvline(cv_scores.mean(), color='red', linestyle='dashed', linewidth=2)\n",
    "plt.xlabel('Accuracy Score')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Cross-validation Scores - Gradient Boosting Model')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nCross-validation Scores Summary:\")\n",
    "print(f\"Mean Accuracy: {cv_scores.mean():.3f} (+/- {cv_scores.std() * 2:.3f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Partial Dependence Plots\n",
    "from sklearn.inspection import partial_dependence\n",
    "\n",
    "def plot_partial_dependence(model, X, features, feature_names):\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    for i, feature in enumerate(features):\n",
    "        plt.subplot(1, len(features), i+1)\n",
    "        pd_result = partial_dependence(model, X, [feature])\n",
    "        plt.plot(pd_result[1][0], pd_result[0][0])\n",
    "        plt.xlabel(feature_names[feature])\n",
    "        plt.ylabel('Partial dependence')\n",
    "    plt.suptitle('Partial Dependence Plots')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Select top 3 important features for partial dependence plots\n",
    "top_features = feature_importance.head(3)['feature'].index.tolist()\n",
    "plot_partial_dependence(best_model.named_steps['gb'], \n",
    "                       X_test, \n",
    "                       top_features, \n",
    "                       selected_features)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
